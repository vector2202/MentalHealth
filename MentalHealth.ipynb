{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23484a1a-bb9d-47fc-a799-f71fe88c47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea855c21-6edb-4a09-afc3-5e595e401f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf08426-61a1-447b-a6cc-6ad74518dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multicapa:\n",
    "    \"\"\"\n",
    "    Red neuronal con tres capas:\n",
    "    \n",
    "    Entrada\n",
    "    Oculta\n",
    "    Salida\n",
    "    \n",
    "    Los parámetros (pesos) que conectan las capas se encuentran en matrices\n",
    "    con los nombres siguientes:\n",
    "    \n",
    "    Entrada -> self.Theta_0 -> Oculta\n",
    "    Oculta  -> self.Theta_1 -> Salida\n",
    "    \"\"\"\n",
    "    def __init__(self, n_entrada, n_ocultas, n_salidas):\n",
    "        \"\"\"\n",
    "        Inicializa la red neuronal, con pesos Theta_0 y Theta_1 aleatorios,\n",
    "        esta implementación debe incluir el uso de sesgos, por lo que éstos\n",
    "        no se cuentan en los parámetros siguientes, puedes incluirlos como\n",
    "        neuronas extra o en sus propias matrices, sólo sé consistente pues\n",
    "        esto afectará tu implementación.\n",
    "        \n",
    "        :param n_entrada: número de datos de entrada (sin contar el sesgo)\n",
    "        :param n_ocultas: número de neuronas ocultas\n",
    "        :param n_salidas: número de nueronas de salida\n",
    "        \"\"\"\n",
    "        self.Theta_0 = np.random.rand(n_entrada + 1, n_ocultas)\n",
    "        self.Theta_1 = np.random.rand(n_ocultas + 1, n_salidas)\n",
    "    \n",
    "    def feed_forward(self, X, vector = None):\n",
    "        \"\"\" Calcula las salidas, dados los datos de entrada en forma de matriz.\n",
    "        Guarda los parámetros siguientes:\n",
    "        A0: activaciones de la capa de entrada, ya con sesgos\n",
    "        Z1: potenciales de la capa oculta, aún sin sesgo\n",
    "        A1: activaciones de la capa oculta, ya con sesgos\n",
    "        Z2: potenciales de la capa de salida\n",
    "        A2: activaciones de la capa de salida\n",
    "        \n",
    "        :param vector: [opcional] se utilizarán los pesos indicados en este\n",
    "                       vector en lugar de los pesos actuales de la red.\n",
    "        \"\"\"\n",
    "        if vector is None:\n",
    "            theta0 = self.Theta_0\n",
    "            theta1 = self.Theta_1\n",
    "        else:\n",
    "            theta0, theta1 = self.reconstruct_matrices(vector)\n",
    "        \n",
    "        self.A0 = np.insert(X, 0, 1, axis=1)\n",
    "        self.Z1 = np.dot(self.A0, theta0)\n",
    "        self.A1 = 1 / (1 + np.exp(-self.Z1))\n",
    "        self.A1 = np.insert(self.A1, 0, 1, axis=1)  # Añadir sesgo a la capa oculta\n",
    "        self.Z2 = np.dot(self.A1, theta1)\n",
    "        self.A2 = 1 / (1 + np.exp(-self.Z2))\n",
    "        return\n",
    "        \n",
    "    def back_propagate(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\" Calcula el error y su gradiente dados los pesos actuales de la red\n",
    "        y los resultados esperados.\n",
    "        \n",
    "        Guarda el error en el atributo self.error y el gradiente en matrices\n",
    "        self.Grad_1 y self.Grad_0, que tienen la misma forma de Theta_0 y Theta_1.\n",
    "        \n",
    "        :param X: matriz de entradas\n",
    "        :param Y: matriz de salidas deseadas\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        self.feed_forward(X)\n",
    "        n = X.shape[0]\n",
    "        error = (1/n) * np.sum(cross_entropy(Y, self.A2)) + (lambda_r / (2*n)) * (np.sum(np.square(self.Theta_0))\n",
    "                                                                                  + np.sum(np.square(self.Theta_1))) \n",
    "        self.error = error\n",
    "        delta_2 = Y - self.A2\n",
    "        delta_1 = np.dot(delta_2, self.Theta_1[1:].T) * np.array(derivada_logistica_atajo(self.A1[:,1:]))\n",
    "\n",
    "        grad_1 = - np.dot(self.A1[:,1:].T, delta_2) / n\n",
    "        sesgo_1 = - np.sum(delta_2, axis = 0, keepdims = True) / n\n",
    "        grad_0 = - np.dot(self.A0[:,1:].T, delta_1) / n\n",
    "        sesgo_0 = - np.sum(delta_1, axis = 0, keepdims = True) / n\n",
    "        \n",
    "        grad_0 += (lambda_r / n) * self.Theta_1[1:]\n",
    "        grad_0 += (lambda_r / n) * self.Theta_0[1:]\n",
    "    \n",
    "        self.Grad_1 = np.concatenate((sesgo_1, grad_1))\n",
    "        self.Grad_0 = np.concatenate((sesgo_0, grad_0))\n",
    "        \n",
    "        return self.error\n",
    "        \n",
    "    def calc_error(self, X, Y, vector, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Calcula el error que se cometería utilizando los pesos en 'vector' en lugar\n",
    "        de los pesos actuales de la red.\n",
    "        \n",
    "        :returns: el error\n",
    "        \"\"\"\n",
    "        theta_0, theta_1 = self.reconstruct_matrices(vector)\n",
    "        self.feed_forward(X, vector)\n",
    "        n = X.shape[0] \n",
    "        return 1/n * np.sum(cross_entropy(Y,self.A2)) + (lambda_r / (2*n)) * (np.sum(np.square(theta_0))\n",
    "                                                                              + np.sum(np.square(theta_1)))\n",
    "    \n",
    "    def vector_weights(self):\n",
    "        \"\"\"\n",
    "        Acomoda a todos los parámetros en las matrices de sesgos y pesos, en un solo vector.\n",
    "        \n",
    "        :returns: vector de parámetros\n",
    "        \"\"\"\n",
    "        return np.concatenate((self.Theta_0.flatten(), self.Theta_1.flatten()))\n",
    "    \n",
    "    def reconstruct_matrices(self, vector):\n",
    "        \"\"\"\n",
    "        Dado un vector, rearma matrices del tamaño de las matrices de sesgos y pesos.\n",
    "        \n",
    "        :returns: matrices de parámetros\n",
    "        \"\"\"\n",
    "        n_oculta, n_entrada = self.Theta_0.shape\n",
    "        n_salida, n_oculta_2 = self.Theta_1.shape\n",
    "        \n",
    "        fin_Theta_0 = n_oculta * n_entrada\n",
    "        fin_Theta_1 = fin_Theta_0 + n_salida * n_oculta_2\n",
    "        \n",
    "        Theta_0 = np.reshape(vector[:fin_Theta_0], (n_oculta, n_entrada))\n",
    "        Theta_1 = np.reshape(vector[fin_Theta_0:fin_Theta_1], (n_salida, n_oculta_2))\n",
    "    \n",
    "        return Theta_0, Theta_1\n",
    "        \n",
    "    def approx_gradient(self, X, Y, lambda_r = 0.0):\n",
    "        \"\"\"\n",
    "        Aproxima el valor del gradiente alrededor de los pesos actuales,\n",
    "        perturbando cada peso, uno por uno hasta estimar la variación alrededor\n",
    "        de cada peso.\n",
    "        \n",
    "        En este método se itera sobre cada peso w:\n",
    "        * Sea w - epsilon -> val1, se calcula el error e1 cometido por la red si w es\n",
    "                                   reemplazado por val1.\n",
    "        * Sea w + epsilon -> val2, se calcula el error e2 cometido por la red si w es\n",
    "                                   reemplazado por val2.\n",
    "        * La parcial correspondiente se estima como (val1 - val2)/(2 * epsilon)\n",
    "        \n",
    "        Este método sólo se utiliza para verificar que backpropagation esté bien\n",
    "        implementado, ya que en la práctica es muy lento y menos preciso.\n",
    "        \n",
    "        :returns: matrices que tienen la misma forma de Theta_0 y Theta_1, donde\n",
    "                  cada entrada es la estimación de la parcial del error con\n",
    "                  respecto al peso correspondiente\n",
    "        \"\"\"\n",
    "        aproximacion_gradiente_0 = np.zeros_like(self.Theta_0)\n",
    "        aproximacion_gradiente_1 = np.zeros_like(self.Theta_1)\n",
    "        epsilon = 0.0001\n",
    "    \n",
    "        for i in range(self.Theta_0.shape[0]):\n",
    "            for j in range(self.Theta_0.shape[1]):\n",
    "                aux = self.Theta_0[i, j]\n",
    "                self.Theta_0[i, j] -= epsilon\n",
    "                self.feed_forward(X)\n",
    "                error1 = self.calc_error(X, Y, self.vector_weights(), lambda_r)   \n",
    "\n",
    "                self.Theta_0[i, j] = aux\n",
    "                self.Theta_0[i, j] += epsilon\n",
    "                self.feed_forward(X)\n",
    "                error2 = self.calc_error(X, Y, self.vector_weights(), lambda_r)\n",
    "\n",
    "                self.Theta_0[i, j] = aux\n",
    "                \n",
    "                aproximacion_gradiente_0[i, j] = (error2 - error1) / (2 * epsilon)\n",
    "    \n",
    "        for i in range(self.Theta_1.shape[0]):\n",
    "            for j in range(self.Theta_1.shape[1]):\n",
    "\n",
    "                aux = self.Theta_1[i, j]\n",
    "                self.Theta_1[i, j] -= epsilon\n",
    "                self.feed_forward(X)\n",
    "                error1 = self.calc_error(X, Y, self.vector_weights(), lambda_r)             \n",
    "\n",
    "                self.Theta_1[i, j] = aux\n",
    "                self.Theta_1[i, j] += epsilon \n",
    "                self.feed_forward(X)\n",
    "                error2 = self.calc_error(X, Y, self.vector_weights(), lambda_r)\n",
    "                \n",
    "                self.Theta_1[i, j] = aux\n",
    "                \n",
    "                aproximacion_gradiente_1[i, j] = (error2 - error1) / (2 * epsilon)\n",
    "    \n",
    "        return aproximacion_gradiente_0, aproximacion_gradiente_1\n",
    "        \n",
    "    def gradient_descent(self, X, Y, alpha, ciclos=10, check_gradient = False, lambda_r = 0.0):\n",
    "        \"\"\" Evalúa y ajusta los pesos de la red, de acuerdo a los datos en X y los resultados\n",
    "        deseados en Y.  Al final grafica el error vs ciclo.  Si el entrenamiento es correcto\n",
    "        el error debe descender por cada iteración (ciclo).\n",
    "        \n",
    "        :param X: datos de entrada\n",
    "        :param Y: salidas deseadas\n",
    "        :param alpha: taza de aprendizaje\n",
    "        :param ciclos: número de veces que se realizarán ajustes para todo el conjunto de datos X\n",
    "        :param check_gradient: se calculará el gradiente con backpropagation y con aproximación por\n",
    "                               perturbaciones, imprimiendo los valores lado a lado para que puedan\n",
    "                               ser comparados.\n",
    "        :param lambda_r: coeficiente de regularización\n",
    "        \"\"\"\n",
    "        errores_back = []\n",
    "        errores_aprox = []\n",
    "        for ciclo in range(ciclos):\n",
    "            #self.feed_forward(X)\n",
    "            self.back_propagate(X, Y, lambda_r)\n",
    "            error_b = self.error\n",
    "            errores_back.append(error_b)\n",
    "            grad0, grad1 = self.Grad_0, self.Grad_1\n",
    "            \n",
    "            if check_gradient:    \n",
    "                aproximacion_gradiente_0, aproximacion_gradiente_1 = self.approx_gradient(X, Y, lambda_r)\n",
    "                error_p = self.calc_error(X,Y,self.vector_weights(), lambda_r)\n",
    "                errores_aprox.append(error_p)\n",
    "                print(f'Gradiente 0: {self.Grad_0}\\n')\n",
    "                print(f'Aproximacion 0: {aproximacion_gradiente_0}\\n')\n",
    "                print(f'Gradiente 1: {self.Grad_1}\\n')\n",
    "                print(f'Aproximación 1: {aproximacion_gradiente_1}\\n')\n",
    "            self.Theta_0 -= alpha * grad0\n",
    "            self.Theta_1 -= alpha * grad1\n",
    "            \n",
    "        plt.plot(np.arange(ciclos), errores_back, label='Error Backpropagation')\n",
    "        if check_gradient:\n",
    "            plt.plot(np.arange(ciclos), errores_aprox, label='Error Aproximación')\n",
    "            plt.legend()\n",
    "        plt.xlabel('Ciclos')\n",
    "        plt.ylabel('Error')\n",
    "        plt.title('Errores vs Ciclo de Entrenamiento')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "            \n",
    "    def print_output(self):\n",
    "        \"\"\"\n",
    "        Muestra en pantalla los valores de salida obtenidos en la última ejecución de feed_forward.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'A2'):\n",
    "            print(\"Valores de salida:\")\n",
    "            print(self.A2)\n",
    "        else:\n",
    "            print(\"Aún no se ha ejecutado feed_forward.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b37b5f6-ef57-4c54-a61c-105eaa7ea3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar y entrenar la red\n",
    "n_entrada = X_train.shape[1]\n",
    "n_ocultas = 10 \n",
    "n_salidas = 1\n",
    "\n",
    "modelo = Multicapa(n_entrada, n_ocultas, n_salidas)\n",
    "modelo.gradient_descent(X_train, y_train.values.reshape(-1, 1), alpha=0.01, ciclos=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacc7b37-f2ce-4f37-a3ba-d073a4834a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo.predict(X_test).flatten()  \n",
    "\n",
    "# Evaluar el modelo\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('Precision:', precision_score(y_test, y_pred))\n",
    "print('Recall:', recall_score(y_test, y_pred))\n",
    "print('F1 Score:', f1_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
